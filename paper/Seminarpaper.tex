% !TeX spellcheck = en_US
% This is LLNCS.DOC the documentation file of
% the LaTeX2e class from Springer-Verlag
% for Lecture Notes in Computer Science, version 2.4
\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage{graphicx} 
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\R}{\mathbb{R}}
%
\begin{document}
\thispagestyle{empty}
\rule{\textwidth}{1pt}
\vspace{2pt}
\begin{flushright}
\Huge
\begin{tabular}{@{}l}
Barriers to the\\
implementation of\\
k-anonymity and\\
related microdata\\
anonymization techniques\\
in a realworld application\\[6pt]

\end{tabular}
\end{flushright}
\rule{\textwidth}{1pt}
\vfill
\title{Barriers to the implementation of k-anonymity and related microdata anonymization techniques in a realworld application}
\author{Andreas Wiegnand, 1878334\\
	Ludwig Schallner, 1850413}
\institute{}
\maketitle
%
%\tableofcontents
\newpage
\setcounter{page}{1}
\section{Introduction}
%
Nowadays data is a key factor in nearly every domain. It is comparable to the gold rush of the \ensuremath{19^{th}.} century \cite{datarevo}. Furthermore, storage space and network increasingly become affordable \cite{sweeney2002k}. 
This is leading to the situation that the created and stored data is often not only useful to the original data holder, but to other researchers. Also, some data is only useful if its get shared with other data and get together analyzed. But those data may contain some personal or sensitive information. Such that the data should only get releases if the privacy is protected \cite{li2006achieving}.\\
\begin{table}[]
	\centering
	\caption{Basic example}
	\label{intro_example}
	\begin{tabular}{@{}llll@{}}
		\toprule
		SSN         & Age & Postcode & Problem         \\ \midrule
		680-90-2665 & 25  & 4568     & procrastination \\
		008-07-4179 & 34  & 4567     & stress          \\
		391-05-7998 & 48  & 4569     & stomach cancer  \\
		078-36-3853 & 39  & 4568     & obesity         \\
		411-71-9290 & 42  & 4561     & stomach ulcers  \\
		527-59-1948 & 27  & 4568     & stress          \\ \bottomrule
	\end{tabular}
\end{table}

Data like table \ref{intro_example} have to get anonymized before release. A very common
technique archive that goal is the so-called k-anonymity. Which goal is to prevent
the possibility that information about the individual gets leaked. This paper is
showing the process of implementing k-anonymity into the real world. In Section
1 we will explain mandatory basic to understand k-anonymity and its purpose.
Which leads to Section 2 the theoretical and heuristically implementation of
k-anonymity. Section 3 will discuss the underlying barriers of k-anonymity.
In Section 4 we will explain to the reader multiple algorithms to implement
k-anonymity and its barriers. A summary of the whole paper will be in the last section
\newpage
\section{Basics}
\subsubsection{Microdata:}
First of all, it should be clear what microdata is, those data is containing records of information about individuals. The upside versus the more known summary or aggregate data is, that microdata is naturally flexible. Everyone who has this data can perform own statistics from that data \cite{microdataweb}.
\subsubsection{Identifier:}
Attributes which can identify the record owner explicitly without any other attribute, for example full name (name and surname), telephone number, social security number. nicht sicher ob noch mehr möglich is\cite{domingo2008critique}
\subsubsection{Quasi-identifier:}
Even though explicit identifier got removed from published data. Attributes which non-explicitly identify the record owner are left. But if they get combined with other non-explicit attributes or other tables, they can reidentify the record owner. In such a case those combination of attributes are called quasi-identifier. For example Gender, Age, Postcode, weight and height \cite{dalenius1986finding}. Such process is shown in figure \ref{quasiidentifier}.
\subsubsection{Sensitive data:}
Data which is useful for example researchers but are private and should be known publicly nor be accessible for outsiders \cite{ldiversity}
\subsubsection{Background-knowledge:}
Because its unknown what the attackers knows, we have to assume additionally to that he have access to table, the attackers knows that the table is generalized (to guarantee k-anonymity). Furthermore the attacks is aware of the domain of the attributes.\\
\textbf{Instance-level background knowledge}
The adversary knows that his target does know specific details about his target. For example Alice (the adversary) knows that Bob do not suffer from some disease, because he does not show the symptoms. In this case the adversary may can conclude what Bob is really suffers from.\\
\textbf{Demographic background knowledge}
Adversary knows for example more general fact for example P(t[condition] = cancer| t[Age] $\geq$ 40). The attacker may use it to interference about records \cite{ldiversity}

\subsubsection{K-Anonymity}
The goal of making a k-anonymized table, is to have at least (k-1) tuples of each identical tuple taking the corresponding quasi-identifiers into account \cite{sweeney2002k,li2006achieving}. For example the 2-anonyminized version of the table \ref{intro_example} of introduction section
\subsubsection{Equivalence class}
Is a set of all tuples with the identical quasi-identifiers of a table \cite{li2006achieving}.
\subsubsection{global recoding/domain generalisation}
This generalization technique is very common, if a attribute value get generalized then all occourences of that value gets replaced by the generalized one \cite{sweeney2002k,sweeney2002achieving,li2006achieving,incognito}. 
\subsubsection{local recoding}
This coding strategies works differently from the above described one. Local recording generalizes attribute values in cells. Because of that this strategies doenst over generalize the table and the data distortion is significantly lower \cite{li2006achieving}. 

\section{Theoretical and heuristically implantation of K-Anonymity}
Another problem we will introduce is, that the producing of k-anonymity of a computational view is an NP-hard problem, like Meyersond and Williams shown.
 
\section{Underlying Barriers}

In the following section, we will show the basic and most challenging barriers to the implementation of k-Anonymity. First, we will show the barrier which appears if you k-anonymize the data, the so-called \textbf{distortion} of data, in some papers it also mentioned as data loss. 
%checked
\subsection{Distortion} 
A basic underlying barrier of k-anonymity is, how to measure if a implantation has been successful or leads to a satisfying result. This can be measured by a simple calculation.
 The \textbf{modification rate} is representing the fraction of cells which got modified within the attribute set of the quasi-identifier \cite{li2006achieving}.
\begin{table}[]
	\centering
	\label{table2}
		\caption{a: original table,b: example for local recording, c: example for domain generalization }
	\begin{tabular}{lllllllllll}
		\cline{1-3} \cline{5-7} \cline{9-11}
		Gender & Birthday   & Problem &  & Gender & Birthday   & Problem &  & Gender & Birthday & Problem \\ \cline{1-3} \cline{5-7} \cline{9-11} 
		male   & 13.08.1962 & stress  &  & male   & 13.08.1962 & stress  &  & *      & 196*     & stress  \\
		male   & 28.10.1967 & obesity &  & male   & 28.10.1967 & obesity &  & *      & 196*     & obesity \\
		male   & 20.01.1977 & stress  &  & *      & 197*       & stress  &  & *      & 197*     & stress  \\
		female & 15.09.1973 & obesity &  & *      & 197*       & obesity &  & *      & 197*     & obesity \\
		female & 15.03.1985 & stress  &  & female & 15.03.1985 & stress  &  & *      & 198*     & stress  \\
		female & 28.05.1986 & obesity &  & female & 28.05.1986 & obesity &  & *      & 198*     & obesity \\ \cline{1-3} \cline{5-7} \cline{9-11}
		\end{tabular}

\end{table}\\
\textbf{Example:} for table2 a, the modification rate is  33,$\overline{33}$\% (4 out of 12 quasi-identifier got changed) for table 2c: its is 100\% (12 out of 12 quasi-identifier got changed). Like this simple example shows the modification rate calculation is a unsatisfying procedure. Because of that the \textbf{weighted hierarchical distance} got introduced by Li, Wong, Fu and Pei. 
To calculate the \textbf{weighted hierarchical distance} of a cell, which got generalized from level p to level q, following formula is used \cite{li2006achieving}.\\
$ WHD (p, q) = \frac{\sum_{j=q+1}^{p} \omega_{j,j-1}}{\sum_{j=2}^{h} \omega_{j,j-1}} $\\
The WHD for the Age, let the hierarchy of birth date be \{D/M/Y, M/Y, Y, 10Y, C/T/G/P, *\}. Where D/M/Y  would be day.month.year, 10Y a 10 years interval and C/T/G/R for Child/Teen/Grownup/Pensioner.\\

\textbf{Example with uniformed weight $w_{j,j-1} = 1$ where $2\leq j \leq h$:} For the above example Birthday gets generalized from D/M/Y to 10Y, which corresponds into $WHD_{age}(6,3) = \frac{3}{5} = 0,6$. For the Gender generalization it would be $WHD_{gender}(2,1) = \frac{1}{1} = 1$. Which means for generalize 5 cells of age from D/M/Y to 10Y one will have the same data distortion as if one generalize 3 cells of Gender from Male/Female to *. This calculation shows a much better way to address the distortion of data than the \textbf{modification rate} but it does not take how near a generalization is to the root (which would be *). BEISPIEL??? \\

\textbf{Example with height weight: $w_{j,j-1} = 1 / (j-1)^{\beta}$ where 2 $\leq$ j $\leq$ h and $\beta = \R \geq$ 1:}
ß would be chosen by the user. For example $\beta = 1$. For $WHD_{age}(6,3) = \frac{0,\overline{33}+0,25+0,20}{1+0.5+0,\overline{33}+0,25+0,20} \sim 0,3431$. For $WHD_{gender}(2,1) = \frac{1}{1} = 1$. The distortion  

\subsubsection{Conclusion}

\subsection{NP Hard}

\subsection{Attacks}

Like Dalenius already mentioned it is absolutely necessary that an attacker, under no circumstances, can learn about whatsoever target if he is studying the published database. Not even if the attacker has background knowledge from any other sources  \cite{Dalenius1977}. Unfortunately like Dwork showed 2006 that such safety is impossible because of background knowledge. For example, if the attacker knows that Bob get paid twice as the average German man and the attacker got access to a database which publishes the average income by German men. The anonymity of Bob is compromised even if Bob's data is not in the database \cite{dwork2011differential}.  
Ab hier noch mal komplett die Attacken überarbeiten, da noch im orginal ursprung!
\subsubsection{Linking data}
A barrier to do the implementation of k-anonymity, the attacker can take another dataset and link both together to get rid off the k-anonymity and infer the real individual. This process is called linking data and was first described by Sweeney\cite{sweeney2002k}. She showed that with a example of health care data from 37 states in the USA. The institute from which she bought the data, insures the anonymity of the individuals. Sweeney purchased the voter registration list for Cambridge Massachusettts and received information of the voters including ZIP code, birth date and gender (non explicit identifier) of each voter. She linked that information with the medical data. It was possible to deanonymize the data  and get ethnicity, visit date, diagnosis, procedure, medication and total charge of some patients \cite{sweeney2002k}. 

\begin{figure}[]
	\centering
	\includegraphics[width=0.7\textwidth]{linkingdata.png}
	\caption{linking data}%
	\label{quasiidentifier}
\end{figure}
You got two datasets A and B. Each dataset got \ensuremath{\langle f_1, ... ,f_n \rangle} features and \ensuremath{\langle r_1, ... ,r_n \rangle} rows.
Each row is then a tuple \ensuremath{r_i} with n features \ensuremath{\langle f_1, ... ,f_n \rangle} describing the individual.
Even tho the data is k-anonimized you can get rid oft he anonymity of the individual by linking the A to B. So if \ensuremath{A \cap B \not \neq \emptyset} it is possible to infer the anonymized individual \cite{sweeney2002k}.
As a result any attacker who know such data (ZIP Code, Birth date and sex) could easily identify with such an attack his victim. For example Peter see his ex-wife at the doctor, most likely he knows her ZIP-Code, Birth date and sex. Therefore he finds out what she is suffer from. 
\subsubsection{Unsorted matching attack against k-anonymity}
There is a possibility of a leak of information, if the release k-anonymity data is in some kind of a sort release. This mean the numerical attributes are descending or ascending sorted and attributes, which be of characters are alphabetical ordered, can give the attacker Information about the sensitive data. To prevent this attack, just get the data into a random order with a pseudo randomized sorting algorithm \cite{sweeney2002k}. As an example take a look at the table 3: matching attack  will give an example on that. If you compare the different release generalized tables you can figure out all quasi identifier of those \cite{sweeney2002k}.
\\...
\begin{table}	
	\caption{matching attack}
	\centering
	\begin{tabular}[t]{|l|l|}		
	\hline
	Age & ZIP   \\ \hline
	2   & 91058 \\ 
	4   & 91058 \\ 
	50  & 27785 \\ 
	52  & 27785 \\ 
	20  & 32105 \\ 
	21  & 32105 \\ 
	31  & 67676 \\ 
	32  & 67676 \\ \hline
		\end{tabular}
	\hfill
	\begin{tabular}[t]{|l|l|}
	\hline
	Age & ZIP   \\ \hline
	*   & 91058 \\
	**   & 91058 \\
	5*  & 27785 \\
	5*  & 27785 \\
	2*  & 32105 \\
	2*  & 32105 \\
	3*  & 67676 \\
	3*  & 67676 \\ \hline  
	\end{tabular}
	\hfill
	\begin{tabular}[t]{|l|l|}
	\hline
	Age & ZIP   \\	\hline
	2   & 91*   \\
	4   & 91*   \\
	50  & 27*   \\
	52  & 27*   \\
	20  & 32*   \\
	21  & 32*   \\
	31  & 67*   \\
	32  & 67*   \\ \hline  
	\end{tabular}
\end{table}
\subsubsection{Complexity of producing k-anonymity}
BRauchen wir das????
Till now we only looked at problems of information leaking and privacy problems for individuals. Data is personal-specific information which is structured as a table in rows and columns. Rows a tuple. The columns are attributes with are a set of values which describe the certain attribute. A tuple specify a person. K-anonymity is about protecting the identity of a person not relationships of companies or governments. So the goal of k-anonymity is, not getting more information by linking the data to external data. The bridge between the data and external data is called "quasi-identifier". Examples for that would be ZIP, gender, birth date etc.. \\
\\
Generalization mean, replacing a value with a less specific but semantic identical value. For example we got a list of forenames of buys, (Achmed, Achilles, Achim). To generalize this names you can just (Ach*,Ach*, Ach*) delete the last chars of the name. So there is a less specific domain and now more generalize through this mapping. Suppression on the other hand means not releasing the value at all.
\section{Algorithm}
\subsection{Clustering}
Needed because data contains categorical values, the methods are not quite effective.
\cite{li2006achieving}
\subsection{The OlA Algorithm }
The OLA Algorithm works in 3 Steps that will now be explained.
1.	For every generalization strategy build a binary search to find all k-anonymous nodes in the different strategies.
2.	For every generalization strategy that includes k-anonymous nodes save the one with the least Information loss in the hole generalization strategy this is referet to an local optimon k-anonymous solution.
3.	Now compare the local optimum solutions to respect of ther information loss. The one with the lowest Information loss of all local optimum solutions is  the global optimum solution.

The most time consuming operation is finding the all the K-anonymous notes with and compare them to each other with respect to their information loss.  To get a better performance at the Programm step 1) the OLA algorithm works with Predictive Tagging that boost the process.  This Tagging take advandage of 2 Theorems of the generalization Latice. That every k-anonymous note in the same generalization lattice on hight n. All notes above n and in the same generalization strategy are also k-anonymous. So the algorithm only have to find the first k-anonymous note in the strategy and tag all above as k-anonymous. 

\subsection{Cloaking Algorithm}
The Cloacking Alortihm tries to produce Anonymity on locasen based data for users of Location Bases Services like Pokemon Go Google Maps and so on. The Cloacking Algorithm is installed on a Trusted thiert Party Server and is the linkig between the User and the Lokation based Service Provider. The Goal of the Alortitgh is to produce K-anonymity of locased based data the definition: k-anonymity prevents such a privacy breach by ensuring that each individual record can only be released if there is at least k − 1 other (distinct) individuals whose associated records are indistinguishable from the former in terms of their quasi-identifier values.

The Attacks to gain Information about the User of Sended the massenges give Inside on the reason for the implementation. Restricted Space Identiﬁcation and Observation Identiﬁcation. Restriced Space Identification happens if you are the only on in at a location and the attack good the background knowlwege after receiving the transmitted messange with the location data he can contuct that you are the one. To prefend this leaking of information the cloacking algorthim works with Spatial Cloaking and Temporal Cloacking. Spatial cloackign refers to the Idee that the coordinates will be boxes and the temporal cloacking refers to the function of dislimit the possibility that someone can infere that you are their at that time. With sending the message with a time interval which is bigger than we thought it would be. 

\section{Releated techniques}
\section{Summary}
Like we saw in Chapter OLA Algorithm. There are possibilities of choosen between different Information Loss metrices which all compute different values to the same k-anonymous node. So the implemenatior have to choose which one fits the most on his data. Different Matrics have pros and contras and have to be compared. Another Problem with Information loss is that you can measure the information loss on your dataset but what would be more interesting the information loss compared to upcoming data mining tool or machine learning application. The information loss metrics can´t know which information is important for the upcoming dataminging step. Like we saw  suppression can also harm the quality of the data and should be choosen waisley. 
The production of k-anonymity is NP – Hard, which ould be shown by balbal and balla in babalbala and .
High Dimensonal Data like at Transposition Data with more than thousand of attributes are in practice not cababale to produce k-anonymity for all attributes. The reason for that is the so called Curse of High Dimensonality which produce this problem.  Possiblites are bounded background knowledge that a User decite which attribute like in Risk Management can be stayes as it is. 
The Attacks are also Barrries because the goal of k-anonymity is to provide anonymity. The attacks get ride of the goal and provide the capability for the attack to gain the indentiy of the user. And the end could be said that there a solutions regarding these kind of attack we proposed in this paper. L-Diversity and T-Closeness will help against the these attacks.
The cloaking algorithm shoves a good example of the connection between anonymity and userbility . On AREA Code examples. The anonmitation of the data let suffer the userbility of the apps. Which comes from the contruction of the Spatial Cloaking and Temporal Cloacking to have enough messages to contrain them and let the be k-anonymous. The Software gives the option that a user of the client can deciede  how much Temporal Cloacking he wants to sacrifice to get anonymity. So the user is at least under the control of how much utility he wants to give up. 

\newpage
\bibliography{literature}
\bibliographystyle{splncs03}

\end{document}


